name: Build and Release llama-server Binaries

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      version:
        description: 'Version tag (e.g., v1.0.0)'
        required: true
        type: string

env:
  LLAMA_CPP_VERSION: b4313  # Update this to match llama.cpp commit/tag

jobs:
  build-matrix:
    name: Build ${{ matrix.platform }}-${{ matrix.arch }}-${{ matrix.backend }}
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        include:
          # Linux x86_64 - CPU, CUDA, ROCm
          - os: ubuntu-22.04
            platform: linux
            arch: x86_64
            backend: cpu
          - os: ubuntu-22.04
            platform: linux
            arch: x86_64
            backend: cuda
            cuda_version: "12.4.0"
          - os: ubuntu-22.04
            platform: linux
            arch: x86_64
            backend: rocm
            rocm_version: "6.0"

          # Linux aarch64 - CPU only
          - os: ubuntu-22.04
            platform: linux
            arch: aarch64
            backend: cpu
            cross: true

          # macOS aarch64 - Metal (combined with CPU)
          - os: macos-14  # M1 runner
            platform: macos
            arch: aarch64
            backend: metal

          # Windows x86_64 - CPU, CUDA
          - os: windows-2022
            platform: windows
            arch: x86_64
            backend: cpu
          - os: windows-2022
            platform: windows
            arch: x86_64
            backend: cuda
            cuda_version: "12.4.0"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # Linux CUDA setup
      - name: Install CUDA (Linux)
        if: matrix.backend == 'cuda' && runner.os == 'Linux'
        uses: Jimver/cuda-toolkit@v0.2.14
        with:
          cuda: ${{ matrix.cuda_version }}
          method: network
          sub-packages: '["nvcc", "cudart"]'

      # Linux ROCm setup
      - name: Install ROCm (Linux)
        if: matrix.backend == 'rocm' && runner.os == 'Linux'
        run: |
          wget https://repo.radeon.com/amdgpu-install/${{ matrix.rocm_version }}/ubuntu/jammy/amdgpu-install_${{ matrix.rocm_version }}.deb
          sudo apt-get update
          sudo apt-get install -y ./amdgpu-install_${{ matrix.rocm_version }}.deb
          sudo amdgpu-install --usecase=rocm --no-dkms -y

      # Windows CUDA setup
      - name: Install CUDA (Windows)
        if: matrix.backend == 'cuda' && runner.os == 'Windows'
        uses: Jimver/cuda-toolkit@v0.2.14
        with:
          cuda: ${{ matrix.cuda_version }}
          method: network
          sub-packages: '["nvcc", "cudart"]'

      # Linux dependencies
      - name: Install Linux dependencies
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake

      # macOS dependencies
      - name: Install macOS dependencies
        if: runner.os == 'macOS'
        run: |
          brew install cmake

      # Windows dependencies (CMake is pre-installed)
      - name: Setup MSVC (Windows)
        if: runner.os == 'Windows'
        uses: ilammy/msvc-dev-cmd@v1

      # Build with CMake
      - name: Build llama-server
        shell: bash
        run: |
          mkdir build
          cd build

          # Configure CMake based on backend
          CMAKE_ARGS="-DCMAKE_BUILD_TYPE=Release -DGGML_STATIC=OFF -DLLAMA_SERVER_ONLY=ON"

          if [ "${{ matrix.backend }}" == "cuda" ]; then
            CMAKE_ARGS="$CMAKE_ARGS -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=52;61;70;75;80;86;89;90"
          elif [ "${{ matrix.backend }}" == "rocm" ]; then
            CMAKE_ARGS="$CMAKE_ARGS -DGGML_HIPBLAS=ON"
          elif [ "${{ matrix.backend }}" == "metal" ]; then
            CMAKE_ARGS="$CMAKE_ARGS -DGGML_METAL=ON"
          fi

          cmake .. $CMAKE_ARGS
          cmake --build . --config Release --target llama-server -j$(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 4)

          echo "Build complete"
          ls -lah bin/ || ls -lah Release/ || ls -lah .

      # Find the built binary
      - name: Find binary path
        id: find_binary
        shell: bash
        run: |
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            # Windows: binary might be in build/bin/Release or build/Release
            BINARY_PATH=$(find build -name "llama-server.exe" -type f | head -n 1)
            BINARY_NAME="llama-server.exe"
          else
            # Unix: binary is in build/bin
            BINARY_PATH=$(find build/bin -name "llama-server" -type f 2>/dev/null | head -n 1)
            if [ -z "$BINARY_PATH" ]; then
              # Fallback: search entire build directory
              BINARY_PATH=$(find build -name "llama-server" -type f ! -name "*.d" | head -n 1)
            fi
            BINARY_NAME="llama-server"
          fi

          if [ -z "$BINARY_PATH" ]; then
            echo "ERROR: Binary not found!"
            find build -type f -name "*llama-server*" || true
            exit 1
          fi

          echo "binary_path=$BINARY_PATH" >> $GITHUB_OUTPUT
          echo "binary_name=$BINARY_NAME" >> $GITHUB_OUTPUT
          echo "Found binary at: $BINARY_PATH"

      # Create release directory structure
      - name: Prepare release archive
        shell: bash
        run: |
          mkdir -p release

          # Copy binary
          cp "${{ steps.find_binary.outputs.binary_path }}" "release/${{ steps.find_binary.outputs.binary_name }}"

          # For GPU builds (CUDA/ROCm): Copy shared libraries produced by the build
          if [[ "${{ matrix.backend }}" == "cuda" || "${{ matrix.backend }}" == "rocm" ]] && [ "${{ runner.os }}" != "Windows" ]; then
            # Find the build output directory where libraries are
            BUILD_LIB_DIR="build/ggml/src"

            if [ -d "$BUILD_LIB_DIR" ]; then
              # Create lib directory for shared libraries
              mkdir -p release/lib

              # Copy ALL .so files and symlinks from build output
              echo "Copying shared libraries from $BUILD_LIB_DIR"
              find "$BUILD_LIB_DIR" -maxdepth 1 \( -name "*.so" -o -name "*.so.*" \) -exec cp -P {} release/lib/ \;

              # Also check build/bin for any libraries
              if [ -d "build/bin" ]; then
                find "build/bin" -maxdepth 1 \( -name "*.so" -o -name "*.so.*" \) -exec cp -P {} release/lib/ \;
              fi

              # List what we copied
              echo "Packaged shared libraries:"
              ls -lh release/lib/ || echo "No libraries found"
            else
              echo "Warning: Expected library directory not found: $BUILD_LIB_DIR"
              echo "Searching for .so files in build directory:"
              find build -name "*.so" -o -name "*.so.*" || true
            fi
          fi

          # Create version file
          echo "llama-server" > release/version.txt
          echo "${{ env.LLAMA_CPP_VERSION }}" >> release/version.txt
          echo "Backend: ${{ matrix.backend }}" >> release/version.txt
          echo "Platform: ${{ matrix.platform }}-${{ matrix.arch }}" >> release/version.txt
          echo "Built: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> release/version.txt

          # Make executable on Unix
          if [ "${{ runner.os }}" != "Windows" ]; then
            chmod +x "release/${{ steps.find_binary.outputs.binary_name }}"
          fi

          echo "Release directory contents:"
          ls -lah release/
          if [ -d "release/lib" ]; then
            echo "Libraries:"
            ls -lah release/lib/
          fi

      # Create archive
      - name: Create archive
        shell: bash
        run: |
          ARCHIVE_NAME="llama-server-${{ matrix.platform }}-${{ matrix.arch }}-${{ matrix.backend }}"

          if [ "${{ runner.os }}" == "Windows" ]; then
            7z a "${ARCHIVE_NAME}.zip" ./release/*
            echo "ARCHIVE_FILE=${ARCHIVE_NAME}.zip" >> $GITHUB_ENV
          else
            tar czf "${ARCHIVE_NAME}.tar.gz" -C release .
            echo "ARCHIVE_FILE=${ARCHIVE_NAME}.tar.gz" >> $GITHUB_ENV
          fi

      # Generate checksum
      - name: Generate checksum
        shell: bash
        run: |
          if [ "${{ runner.os }}" == "Windows" ]; then
            certutil -hashfile "$ARCHIVE_FILE" SHA256 > "${ARCHIVE_FILE}.sha256"
          else
            shasum -a 256 "$ARCHIVE_FILE" > "${ARCHIVE_FILE}.sha256"
          fi

          cat "${ARCHIVE_FILE}.sha256"

      # Upload artifacts
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-server-${{ matrix.platform }}-${{ matrix.arch }}-${{ matrix.backend }}
          path: |
            ${{ env.ARCHIVE_FILE }}
            ${{ env.ARCHIVE_FILE }}.sha256
          retention-days: 7

  create-release:
    name: Create GitHub Release
    needs: build-matrix
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/')

    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Prepare release assets
        run: |
          mkdir -p release-assets
          find artifacts -type f \( -name "*.tar.gz" -o -name "*.zip" -o -name "*.sha256" \) -exec cp {} release-assets/ \;
          ls -lah release-assets/

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          files: release-assets/*
          draft: false
          prerelease: false
          generate_release_notes: true
          body: |
            ## llama-server Release ${{ github.ref_name }}

            Based on llama.cpp version: `${{ env.LLAMA_CPP_VERSION }}`

            ### Supported Platforms and Backends

            - **Linux x86_64**: CPU, CUDA, ROCm
            - **Linux aarch64**: CPU
            - **macOS aarch64**: Metal
            - **Windows x86_64**: CPU, CUDA

            ### Installation

            1. Download the appropriate archive for your platform and GPU backend
            2. Extract the archive
            3. For GPU builds on Linux, set `LD_LIBRARY_PATH` to include the `lib/` directory:
               ```bash
               export LD_LIBRARY_PATH="$PWD/lib:$LD_LIBRARY_PATH"
               ./llama-server --help
               ```
            4. The `llama-server` binary is ready to use

            ### Checksums

            SHA256 checksums are provided for each archive file. Verify downloads with:
            ```bash
            shasum -a 256 -c llama-server-*.sha256
            ```
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
