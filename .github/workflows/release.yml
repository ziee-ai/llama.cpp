name: Build and Release llama-server Binaries

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      version:
        description: 'Version tag (e.g., v1.0.0)'
        required: true
        type: string

env:
  LLAMA_CPP_VERSION: b4313  # Update this to match llama.cpp commit/tag

jobs:
  build-matrix:
    name: Build ${{ matrix.platform }}-${{ matrix.arch }}-${{ matrix.backend }}
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        include:
          # Linux x86_64 - CPU only (CUDA and ROCm are in separate jobs below)
          - os: ubuntu-22.04
            platform: linux
            arch: x86_64
            backend: cpu

          # Linux aarch64 - CPU only
          - os: ubuntu-22.04
            platform: linux
            arch: aarch64
            backend: cpu
            cross: true

          # macOS aarch64 - Metal (combined with CPU)
          - os: macos-14  # M1 runner
            platform: macos
            arch: aarch64
            backend: metal

          # Windows x86_64 - CPU, CUDA
          - os: windows-2022
            platform: windows
            arch: x86_64
            backend: cpu
          - os: windows-2022
            platform: windows
            arch: x86_64
            backend: cuda
            cuda_version: "12.4.0"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # ccache for faster rebuilds
      - name: ccache
        uses: hendrikmuhs/ccache-action@v1.2
        with:
          key: ${{ matrix.platform }}-${{ matrix.arch }}-${{ matrix.backend }}
          max-size: 500M
          update-package-index: true

      # Linux dependencies
      - name: Install Linux dependencies
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake libcurl4-openssl-dev

      # macOS dependencies
      - name: Install macOS dependencies
        if: runner.os == 'macOS'
        run: |
          brew install cmake

      # Windows dependencies (CMake is pre-installed)
      - name: Setup MSVC (Windows)
        if: runner.os == 'Windows'
        uses: ilammy/msvc-dev-cmd@v1

      # Build with CMake
      - name: Build llama-server
        shell: bash
        run: |
          mkdir build
          cd build

          # Base CMake configuration with optimization flags
          CMAKE_ARGS="-DCMAKE_BUILD_TYPE=Release -DGGML_STATIC=OFF -DLLAMA_SERVER_ONLY=ON"

          # Portability and compatibility flags
          CMAKE_ARGS="$CMAKE_ARGS -DGGML_NATIVE=OFF"              # Don't optimize for build machine CPU
          CMAKE_ARGS="$CMAKE_ARGS -DGGML_BACKEND_DL=ON"           # Dynamic backend loading

          # Platform-specific RPATH for automatic library discovery
          if [ "${{ runner.os }}" == "Linux" ]; then
            CMAKE_ARGS="$CMAKE_ARGS -DCMAKE_INSTALL_RPATH='\$ORIGIN'"
            CMAKE_ARGS="$CMAKE_ARGS -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON"
            # Multi-CPU variant support for x86_64
            if [ "${{ matrix.arch }}" == "x86_64" ]; then
              CMAKE_ARGS="$CMAKE_ARGS -DGGML_CPU_ALL_VARIANTS=ON"
            fi
          elif [ "${{ runner.os }}" == "macOS" ]; then
            CMAKE_ARGS="$CMAKE_ARGS -DCMAKE_INSTALL_RPATH='@loader_path'"
            CMAKE_ARGS="$CMAKE_ARGS -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON"
            # Metal-specific optimizations
            if [ "${{ matrix.backend }}" == "metal" ]; then
              CMAKE_ARGS="$CMAKE_ARGS -DGGML_METAL_USE_BF16=ON"
              CMAKE_ARGS="$CMAKE_ARGS -DGGML_METAL_EMBED_LIBRARY=ON"
            fi
          fi

          # Backend-specific configuration
          if [ "${{ matrix.backend }}" == "cuda" ]; then
            CMAKE_ARGS="$CMAKE_ARGS -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=52;61;70;75;80;86;89;90"
          elif [ "${{ matrix.backend }}" == "rocm" ]; then
            CMAKE_ARGS="$CMAKE_ARGS -DGGML_HIPBLAS=ON"
          elif [ "${{ matrix.backend }}" == "metal" ]; then
            CMAKE_ARGS="$CMAKE_ARGS -DGGML_METAL=ON"
          fi

          # Enable ccache for faster rebuilds
          export CMAKE_C_COMPILER_LAUNCHER=ccache
          export CMAKE_CXX_COMPILER_LAUNCHER=ccache

          cmake .. $CMAKE_ARGS
          cmake --build . --config Release --target llama-server -j$(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 4)

          echo "Build complete"
          ls -lah bin/ || ls -lah Release/ || ls -lah .

      # Find the built binary
      - name: Find binary path
        id: find_binary
        shell: bash
        run: |
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            # Windows: binary might be in build/bin/Release or build/Release
            BINARY_PATH=$(find build -name "llama-server.exe" -type f | head -n 1)
            BINARY_NAME="llama-server.exe"
          else
            # Unix: binary is in build/bin
            BINARY_PATH=$(find build/bin -name "llama-server" -type f 2>/dev/null | head -n 1)
            if [ -z "$BINARY_PATH" ]; then
              # Fallback: search entire build directory
              BINARY_PATH=$(find build -name "llama-server" -type f ! -name "*.d" | head -n 1)
            fi
            BINARY_NAME="llama-server"
          fi

          if [ -z "$BINARY_PATH" ]; then
            echo "ERROR: Binary not found!"
            find build -type f -name "*llama-server*" || true
            exit 1
          fi

          echo "binary_path=$BINARY_PATH" >> $GITHUB_OUTPUT
          echo "binary_name=$BINARY_NAME" >> $GITHUB_OUTPUT
          echo "Found binary at: $BINARY_PATH"

      # Create release directory structure
      - name: Prepare release archive
        shell: bash
        run: |
          mkdir -p release

          # Copy binary
          cp "${{ steps.find_binary.outputs.binary_path }}" "release/${{ steps.find_binary.outputs.binary_name }}"

          # For builds with dynamic backends: Copy shared libraries to same directory as binary
          # RPATH is set to $ORIGIN (Linux) or @loader_path (macOS), so libs must be alongside binary
          if [ -d "build/bin/Release" ]; then
            BUILD_LIB_DIR="build/bin/Release"
          else
            BUILD_LIB_DIR="build/bin"
          fi

          echo "Copying shared libraries from $BUILD_LIB_DIR"

          if [ "${{ runner.os }}" == "Windows" ]; then
            # Windows: Copy all DLLs
            find "$BUILD_LIB_DIR" -maxdepth 1 -name "*.dll" -exec cp {} release/ \;
            echo "Packaged DLLs:"
            ls -lh release/*.dll 2>/dev/null || echo "No DLLs found"

          elif [ "${{ runner.os }}" == "macOS" ]; then
            # macOS: Copy dylibs (resolved, not symlinks) and backend .so files
            for lib in libllama libggml libggml-base libmtmd; do
              if [ -f "$BUILD_LIB_DIR/${lib}.dylib" ]; then
                cp -H "$BUILD_LIB_DIR/${lib}.dylib" "release/"
              fi
            done

            # Copy backend plugins (.so files)
            find "$BUILD_LIB_DIR" -maxdepth 1 -name "*.so" -exec cp {} release/ \;

            # Copy Metal shader files for runtime compilation (only for Metal builds)
            if [ "${{ matrix.backend }}" == "metal" ]; then
              for shader in ggml-metal.metal ggml-common.h ggml-metal-impl.h; do
                if [ -f "$BUILD_LIB_DIR/$shader" ]; then
                  cp "$BUILD_LIB_DIR/$shader" "release/"
                fi
              done
            fi

            # Copy license files
            find "$BUILD_LIB_DIR" -maxdepth 1 -name "LICENSE-*" -exec cp {} release/ \;

            echo "Packaged libraries:"
            ls -lh release/*.dylib release/*.so 2>/dev/null || echo "No libraries found"

          else
            # Linux: Copy .so files and symlinks
            find "$BUILD_LIB_DIR" -maxdepth 1 \( -name "*.so" -o -name "*.so.*" \) -exec cp -P {} release/ \;

            # Also check build/ggml/src for additional libraries
            if [ -d "build/ggml/src" ]; then
              find "build/ggml/src" -maxdepth 1 \( -name "*.so" -o -name "*.so.*" \) -exec cp -P {} release/ \;
            fi

            echo "Packaged libraries:"
            ls -lh release/*.so 2>/dev/null || echo "No libraries found"
          fi

          # Create version file
          echo "llama-server" > release/version.txt
          echo "${{ env.LLAMA_CPP_VERSION }}" >> release/version.txt
          echo "Backend: ${{ matrix.backend }}" >> release/version.txt
          echo "Platform: ${{ matrix.platform }}-${{ matrix.arch }}" >> release/version.txt
          echo "Built: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> release/version.txt

          # Make executable on Unix
          if [ "${{ runner.os }}" != "Windows" ]; then
            chmod +x "release/${{ steps.find_binary.outputs.binary_name }}"
          fi

          echo "Release directory contents:"
          ls -lah release/

      # Create archive
      - name: Create archive
        shell: bash
        run: |
          ARCHIVE_NAME="llama-server-${{ matrix.platform }}-${{ matrix.arch }}-${{ matrix.backend }}"

          if [ "${{ runner.os }}" == "Windows" ]; then
            7z a "${ARCHIVE_NAME}.zip" ./release/*
            echo "ARCHIVE_FILE=${ARCHIVE_NAME}.zip" >> $GITHUB_ENV
          else
            tar czf "${ARCHIVE_NAME}.tar.gz" -C release .
            echo "ARCHIVE_FILE=${ARCHIVE_NAME}.tar.gz" >> $GITHUB_ENV
          fi

      # Generate checksum
      - name: Generate checksum
        shell: bash
        run: |
          if [ "${{ runner.os }}" == "Windows" ]; then
            certutil -hashfile "$ARCHIVE_FILE" SHA256 > "${ARCHIVE_FILE}.sha256"
          else
            shasum -a 256 "$ARCHIVE_FILE" > "${ARCHIVE_FILE}.sha256"
          fi

          cat "${ARCHIVE_FILE}.sha256"

      # Upload artifacts
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-server-${{ matrix.platform }}-${{ matrix.arch }}-${{ matrix.backend }}
          path: |
            ${{ env.ARCHIVE_FILE }}
            ${{ env.ARCHIVE_FILE }}.sha256
          retention-days: 7

  # Separate job for CUDA (requires Docker container)
  build-cuda:
    name: Build linux-x86_64-cuda
    runs-on: ubuntu-22.04
    container: nvidia/cuda:12.4.0-devel-ubuntu22.04

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          apt-get update
          apt-get install -y build-essential git cmake libcurl4-openssl-dev curl

      - name: Install Node.js
        run: |
          curl -fsSL https://deb.nodesource.com/setup_18.x | bash -
          apt-get install -y nodejs

      - name: ccache
        uses: hendrikmuhs/ccache-action@v1.2
        with:
          key: linux-x86_64-cuda
          max-size: 500M

      - name: Build llama-server
        run: |
          mkdir build
          cd build

          # Base CMake configuration with optimization flags
          CMAKE_ARGS="-DCMAKE_BUILD_TYPE=Release -DGGML_STATIC=OFF -DLLAMA_SERVER_ONLY=ON"

          # Portability and compatibility flags
          CMAKE_ARGS="$CMAKE_ARGS -DGGML_NATIVE=OFF"              # Don't optimize for build machine CPU
          CMAKE_ARGS="$CMAKE_ARGS -DGGML_BACKEND_DL=ON"           # Dynamic backend loading
          CMAKE_ARGS="$CMAKE_ARGS -DGGML_CPU_ALL_VARIANTS=ON"    # Multi-CPU variant support

          # RPATH for automatic library discovery
          CMAKE_ARGS="$CMAKE_ARGS -DCMAKE_INSTALL_RPATH='\$ORIGIN'"
          CMAKE_ARGS="$CMAKE_ARGS -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON"

          # CUDA backend
          CMAKE_ARGS="$CMAKE_ARGS -DGGML_CUDA=ON"
          CMAKE_ARGS="$CMAKE_ARGS -DCMAKE_CUDA_ARCHITECTURES=52;61;70;75;80;86;89;90"

          # Enable ccache
          export CMAKE_C_COMPILER_LAUNCHER=ccache
          export CMAKE_CXX_COMPILER_LAUNCHER=ccache

          cmake .. $CMAKE_ARGS
          cmake --build . --config Release --target llama-server -j$(nproc)

          echo "Build complete"
          ls -lah bin/

      - name: Find binary path
        id: find_binary
        run: |
          BINARY_PATH=$(find build/bin -name "llama-server" -type f | head -n 1)
          if [ -z "$BINARY_PATH" ]; then
            echo "ERROR: Binary not found!"
            exit 1
          fi
          echo "binary_path=$BINARY_PATH" >> $GITHUB_OUTPUT
          echo "Found binary at: $BINARY_PATH"

      - name: Prepare release archive
        run: |
          mkdir -p release

          # Copy binary
          cp "${{ steps.find_binary.outputs.binary_path }}" "release/llama-server"

          # Copy shared libraries
          BUILD_LIB_DIR="build/ggml/src"
          if [ -d "$BUILD_LIB_DIR" ]; then
            mkdir -p release/lib
            find "$BUILD_LIB_DIR" -maxdepth 1 \( -name "*.so" -o -name "*.so.*" \) -exec cp -P {} release/lib/ \;
            if [ -d "build/bin" ]; then
              find "build/bin" -maxdepth 1 \( -name "*.so" -o -name "*.so.*" \) -exec cp -P {} release/lib/ \;
            fi
            echo "Packaged shared libraries:"
            ls -lh release/lib/
          fi

          # Create version file
          echo "llama-server" > release/version.txt
          echo "${{ env.LLAMA_CPP_VERSION }}" >> release/version.txt
          echo "Backend: cuda" >> release/version.txt
          echo "Platform: linux-x86_64" >> release/version.txt
          echo "Built: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> release/version.txt

          chmod +x "release/llama-server"

          echo "Release directory contents:"
          ls -lah release/

      - name: Create archive
        run: |
          tar czf "llama-server-linux-x86_64-cuda.tar.gz" -C release .
          echo "ARCHIVE_FILE=llama-server-linux-x86_64-cuda.tar.gz" >> $GITHUB_ENV

      - name: Generate checksum
        run: |
          shasum -a 256 "$ARCHIVE_FILE" > "${ARCHIVE_FILE}.sha256"
          cat "${ARCHIVE_FILE}.sha256"

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-server-linux-x86_64-cuda
          path: |
            ${{ env.ARCHIVE_FILE }}
            ${{ env.ARCHIVE_FILE }}.sha256
          retention-days: 7

  # Separate job for ROCm (requires Docker container)
  build-rocm:
    name: Build linux-x86_64-rocm
    runs-on: ubuntu-22.04
    container: rocm/dev-ubuntu-22.04:6.1.2

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential git cmake rocblas-dev hipblas-dev libssl-dev rocwmma-dev curl libcurl4-openssl-dev

      - name: Install Node.js
        run: |
          curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
          sudo apt-get install -y nodejs

      - name: ccache
        uses: hendrikmuhs/ccache-action@v1.2
        with:
          key: linux-x86_64-rocm
          max-size: 500M

      - name: Build llama-server
        run: |
          mkdir build
          cd build

          # Base CMake configuration with optimization flags
          CMAKE_ARGS="-DCMAKE_BUILD_TYPE=Release -DGGML_STATIC=OFF -DLLAMA_SERVER_ONLY=ON"

          # Portability and compatibility flags
          CMAKE_ARGS="$CMAKE_ARGS -DGGML_NATIVE=OFF"              # Don't optimize for build machine CPU
          CMAKE_ARGS="$CMAKE_ARGS -DGGML_BACKEND_DL=ON"           # Dynamic backend loading
          CMAKE_ARGS="$CMAKE_ARGS -DGGML_CPU_ALL_VARIANTS=ON"    # Multi-CPU variant support

          # RPATH for automatic library discovery
          CMAKE_ARGS="$CMAKE_ARGS -DCMAKE_INSTALL_RPATH='\$ORIGIN'"
          CMAKE_ARGS="$CMAKE_ARGS -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON"

          # ROCm backend
          CMAKE_ARGS="$CMAKE_ARGS -DGGML_HIPBLAS=ON"

          # Enable ccache
          export CMAKE_C_COMPILER_LAUNCHER=ccache
          export CMAKE_CXX_COMPILER_LAUNCHER=ccache

          cmake .. $CMAKE_ARGS
          cmake --build . --config Release --target llama-server -j$(nproc)

          echo "Build complete"
          ls -lah bin/

      - name: Find binary path
        id: find_binary
        run: |
          BINARY_PATH=$(find build/bin -name "llama-server" -type f | head -n 1)
          if [ -z "$BINARY_PATH" ]; then
            echo "ERROR: Binary not found!"
            exit 1
          fi
          echo "binary_path=$BINARY_PATH" >> $GITHUB_OUTPUT
          echo "Found binary at: $BINARY_PATH"

      - name: Prepare release archive
        run: |
          mkdir -p release

          # Copy binary
          cp "${{ steps.find_binary.outputs.binary_path }}" "release/llama-server"

          # Copy shared libraries
          BUILD_LIB_DIR="build/ggml/src"
          if [ -d "$BUILD_LIB_DIR" ]; then
            mkdir -p release/lib
            find "$BUILD_LIB_DIR" -maxdepth 1 \( -name "*.so" -o -name "*.so.*" \) -exec cp -P {} release/lib/ \;
            if [ -d "build/bin" ]; then
              find "build/bin" -maxdepth 1 \( -name "*.so" -o -name "*.so.*" \) -exec cp -P {} release/lib/ \;
            fi
            echo "Packaged shared libraries:"
            ls -lh release/lib/
          fi

          # Create version file
          echo "llama-server" > release/version.txt
          echo "${{ env.LLAMA_CPP_VERSION }}" >> release/version.txt
          echo "Backend: rocm" >> release/version.txt
          echo "Platform: linux-x86_64" >> release/version.txt
          echo "Built: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> release/version.txt

          chmod +x "release/llama-server"

          echo "Release directory contents:"
          ls -lah release/

      - name: Create archive
        run: |
          tar czf "llama-server-linux-x86_64-rocm.tar.gz" -C release .
          echo "ARCHIVE_FILE=llama-server-linux-x86_64-rocm.tar.gz" >> $GITHUB_ENV

      - name: Generate checksum
        run: |
          shasum -a 256 "$ARCHIVE_FILE" > "${ARCHIVE_FILE}.sha256"
          cat "${ARCHIVE_FILE}.sha256"

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-server-linux-x86_64-rocm
          path: |
            ${{ env.ARCHIVE_FILE }}
            ${{ env.ARCHIVE_FILE }}.sha256
          retention-days: 7

  create-release:
    name: Create GitHub Release
    needs: [build-matrix, build-cuda, build-rocm]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/')

    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Prepare release assets
        run: |
          mkdir -p release-assets
          find artifacts -type f \( -name "*.tar.gz" -o -name "*.zip" -o -name "*.sha256" \) -exec cp {} release-assets/ \;
          ls -lah release-assets/

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          files: release-assets/*
          draft: false
          prerelease: false
          generate_release_notes: true
          body: |
            ## llama-server Release ${{ github.ref_name }}

            Based on llama.cpp version: `${{ env.LLAMA_CPP_VERSION }}`

            ### Supported Platforms and Backends

            - **Linux x86_64**: CPU, CUDA, ROCm
            - **Linux aarch64**: CPU
            - **macOS aarch64**: Metal
            - **Windows x86_64**: CPU, CUDA

            ### Installation

            1. Download the appropriate archive for your platform and GPU backend
            2. Extract the archive
            3. The `llama-server` binary is ready to use:
               ```bash
               ./llama-server --help
               ```

            **Note**: GPU builds include shared libraries in the `lib/` directory. These are automatically discovered via RPATH - no need to set `LD_LIBRARY_PATH`.

            ### Checksums

            SHA256 checksums are provided for each archive file. Verify downloads with:
            ```bash
            shasum -a 256 -c llama-server-*.sha256
            ```
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
